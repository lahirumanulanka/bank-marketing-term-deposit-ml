{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Data Merging & Preprocessing\n",
    "\n",
    "## Objective\n",
    "This notebook covers:\n",
    "1. Loading both dataset variants (bank-full.csv and bank-additional-full.csv)\n",
    "2. Understanding column differences between datasets\n",
    "3. Aligning columns by adding missing economic features\n",
    "4. Merging datasets into a single comprehensive dataset\n",
    "5. Saving the merged dataset for further analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Datasets\n",
    "\n",
    "We have two datasets to work with:\n",
    "1. **bank-full.csv**: Original dataset (16 features + target)\n",
    "2. **bank-additional-full.csv**: Enhanced dataset (20 features + target)\n",
    "\n",
    "Both use semicolon (`;`) as delimiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "bank_full_path = '../dataset/bank/bank-full.csv'\n",
    "bank_additional_path = '../dataset/bank-additional/bank-additional-full.csv'\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading bank-full.csv...\")\n",
    "df_bank = pd.read_csv(bank_full_path, sep=';')\n",
    "print(f\"✓ Loaded {len(df_bank):,} rows\")\n",
    "\n",
    "print(\"\\nLoading bank-additional-full.csv...\")\n",
    "df_additional = pd.read_csv(bank_additional_path, sep=';')\n",
    "print(f\"✓ Loaded {len(df_additional):,} rows\")\n",
    "\n",
    "print(f\"\\nTotal potential rows after merge: {len(df_bank) + len(df_additional):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore Dataset Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check basic info for bank-full.csv\n",
    "print(\"=\" * 80)\n",
    "print(\"BANK-FULL.CSV STRUCTURE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Shape: {df_bank.shape}\")\n",
    "print(f\"\\nColumns ({len(df_bank.columns)}):\")\n",
    "print(df_bank.columns.tolist())\n",
    "print(\"\\nFirst 3 rows:\")\n",
    "df_bank.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check basic info for bank-additional-full.csv\n",
    "print(\"=\" * 80)\n",
    "print(\"BANK-ADDITIONAL-FULL.CSV STRUCTURE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Shape: {df_additional.shape}\")\n",
    "print(f\"\\nColumns ({len(df_additional.columns)}):\")\n",
    "print(df_additional.columns.tolist())\n",
    "print(\"\\nFirst 3 rows:\")\n",
    "df_additional.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Identify Column Differences\n",
    "\n",
    "Let's identify which columns exist in one dataset but not the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to sets for comparison\n",
    "cols_bank = set(df_bank.columns)\n",
    "cols_additional = set(df_additional.columns)\n",
    "\n",
    "# Find differences\n",
    "only_in_bank = cols_bank - cols_additional\n",
    "only_in_additional = cols_additional - cols_bank\n",
    "common_cols = cols_bank & cols_additional\n",
    "\n",
    "print(\"Column Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nCommon columns: {len(common_cols)}\")\n",
    "print(sorted(common_cols))\n",
    "\n",
    "print(f\"\\n\\nOnly in bank-full.csv: {len(only_in_bank)}\")\n",
    "if only_in_bank:\n",
    "    print(sorted(only_in_bank))\n",
    "else:\n",
    "    print(\"None\")\n",
    "\n",
    "print(f\"\\n\\nOnly in bank-additional-full.csv: {len(only_in_additional)}\")\n",
    "if only_in_additional:\n",
    "    print(sorted(only_in_additional))\n",
    "else:\n",
    "    print(\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Understand Key Differences\n",
    "\n",
    "The main differences between the datasets:\n",
    "\n",
    "1. **bank-full.csv** has:\n",
    "   - `day`: Last contact day of the month (numeric)\n",
    "   - `balance`: Average yearly balance in euros (numeric)\n",
    "\n",
    "2. **bank-additional-full.csv** has:\n",
    "   - `day_of_week`: Last contact day of the week (categorical)\n",
    "   - **5 Economic indicators**:\n",
    "     - `emp.var.rate`: Employment variation rate\n",
    "     - `cons.price.idx`: Consumer price index\n",
    "     - `cons.conf.idx`: Consumer confidence index\n",
    "     - `euribor3m`: Euribor 3 month rate\n",
    "     - `nr.employed`: Number of employees\n",
    "\n",
    "Note: `balance` field is NOT in bank-additional dataset (privacy reasons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types\n",
    "print(\"Data Types - bank-full.csv:\")\n",
    "print(df_bank.dtypes)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nData Types - bank-additional-full.csv:\")\n",
    "print(df_additional.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Strategy for Merging\n",
    "\n",
    "To merge these datasets properly, we need to:\n",
    "\n",
    "1. **Align columns**: Create a union of all columns\n",
    "2. **Handle missing columns**:\n",
    "   - Add 5 economic features to `df_bank` with `NaN` values\n",
    "   - Add `day_of_week` to `df_bank` with `NaN` or derived from `day` if possible\n",
    "   - Add `balance` and `day` to `df_additional` with `NaN`\n",
    "3. **Concatenate**: Stack the aligned dataframes vertically\n",
    "4. **Add source indicator**: Track which dataset each row came from\n",
    "\n",
    "This approach preserves all available information while maintaining data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of economic indicators to add to df_bank\n",
    "economic_features = ['emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed']\n",
    "\n",
    "# Add missing columns to df_bank\n",
    "df_bank_aligned = df_bank.copy()\n",
    "\n",
    "print(\"Adding missing columns to bank-full.csv...\")\n",
    "for col in economic_features:\n",
    "    df_bank_aligned[col] = np.nan\n",
    "    print(f\"  ✓ Added '{col}' with NaN values\")\n",
    "\n",
    "# Add day_of_week to df_bank with NaN (we don't have this information)\n",
    "df_bank_aligned['day_of_week'] = np.nan\n",
    "print(f\"  ✓ Added 'day_of_week' with NaN values\")\n",
    "\n",
    "print(f\"\\nAligned df_bank shape: {df_bank_aligned.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add missing columns to df_additional\n",
    "df_additional_aligned = df_additional.copy()\n",
    "\n",
    "print(\"Adding missing columns to bank-additional-full.csv...\")\n",
    "\n",
    "# Add 'balance' and 'day' which exist in bank-full but not in bank-additional\n",
    "if 'balance' not in df_additional_aligned.columns:\n",
    "    df_additional_aligned['balance'] = np.nan\n",
    "    print(f\"  ✓ Added 'balance' with NaN values\")\n",
    "\n",
    "if 'day' not in df_additional_aligned.columns:\n",
    "    df_additional_aligned['day'] = np.nan\n",
    "    print(f\"  ✓ Added 'day' with NaN values\")\n",
    "\n",
    "print(f\"\\nAligned df_additional shape: {df_additional_aligned.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify both dataframes now have the same columns\n",
    "cols_bank_aligned = set(df_bank_aligned.columns)\n",
    "cols_additional_aligned = set(df_additional_aligned.columns)\n",
    "\n",
    "print(\"Column alignment check:\")\n",
    "print(f\"df_bank_aligned columns: {len(cols_bank_aligned)}\")\n",
    "print(f\"df_additional_aligned columns: {len(cols_additional_aligned)}\")\n",
    "print(f\"\\nColumns match: {cols_bank_aligned == cols_additional_aligned}\")\n",
    "\n",
    "if cols_bank_aligned != cols_additional_aligned:\n",
    "    print(\"\\nMissing in df_bank_aligned:\", cols_additional_aligned - cols_bank_aligned)\n",
    "    print(\"Missing in df_additional_aligned:\", cols_bank_aligned - cols_additional_aligned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Add Source Tracking\n",
    "\n",
    "Before merging, let's add a column to track which dataset each row came from. This will be useful for analysis later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add source column\n",
    "df_bank_aligned['data_source'] = 'bank-full'\n",
    "df_additional_aligned['data_source'] = 'bank-additional'\n",
    "\n",
    "print(\"Added 'data_source' column to track origin of each row\")\n",
    "print(f\"\\ndf_bank_aligned: {(df_bank_aligned['data_source'] == 'bank-full').sum():,} rows marked as 'bank-full'\")\n",
    "print(f\"df_additional_aligned: {(df_additional_aligned['data_source'] == 'bank-additional').sum():,} rows marked as 'bank-additional'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Merge Datasets\n",
    "\n",
    "Now we can concatenate the aligned dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure column order is the same\n",
    "# Sort columns alphabetically for consistency, but keep 'y' at the end\n",
    "all_cols = sorted([col for col in df_bank_aligned.columns if col != 'y'])\n",
    "all_cols.append('y')  # Target variable at the end\n",
    "\n",
    "df_bank_aligned = df_bank_aligned[all_cols]\n",
    "df_additional_aligned = df_additional_aligned[all_cols]\n",
    "\n",
    "print(\"Column order aligned\")\n",
    "print(f\"Columns: {all_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate datasets\n",
    "print(\"Merging datasets...\")\n",
    "df_merged = pd.concat([df_bank_aligned, df_additional_aligned], axis=0, ignore_index=True)\n",
    "\n",
    "print(f\"\\n✓ Merge complete!\")\n",
    "print(f\"  Total rows: {len(df_merged):,}\")\n",
    "print(f\"  Total columns: {len(df_merged.columns)}\")\n",
    "print(f\"  From bank-full: {(df_merged['data_source'] == 'bank-full').sum():,}\")\n",
    "print(f\"  From bank-additional: {(df_merged['data_source'] == 'bank-additional').sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Verify Merged Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information\n",
    "print(\"Merged Dataset Summary:\")\n",
    "print(\"=\" * 80)\n",
    "df_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check first few rows\n",
    "print(\"First 5 rows from bank-full dataset:\")\n",
    "df_merged[df_merged['data_source'] == 'bank-full'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check first few rows from bank-additional dataset\n",
    "print(\"First 5 rows from bank-additional dataset:\")\n",
    "df_merged[df_merged['data_source'] == 'bank-additional'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values by source\n",
    "print(\"Missing values analysis by source:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for source in ['bank-full', 'bank-additional']:\n",
    "    print(f\"\\n{source.upper()}:\")\n",
    "    df_source = df_merged[df_merged['data_source'] == source]\n",
    "    missing = df_source.isnull().sum()\n",
    "    missing = missing[missing > 0].sort_values(ascending=False)\n",
    "    if len(missing) > 0:\n",
    "        print(missing)\n",
    "    else:\n",
    "        print(\"No missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check target variable distribution\n",
    "print(\"Target variable distribution:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nOverall:\")\n",
    "print(df_merged['y'].value_counts())\n",
    "print(f\"\\nPercentage:\")\n",
    "print(df_merged['y'].value_counts(normalize=True) * 100)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nBy data source:\")\n",
    "for source in ['bank-full', 'bank-additional']:\n",
    "    print(f\"\\n{source}:\")\n",
    "    df_source = df_merged[df_merged['data_source'] == source]\n",
    "    print(df_source['y'].value_counts())\n",
    "    print(f\"Percentage:\")\n",
    "    print(df_source['y'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric columns statistics\n",
    "print(\"Numeric Features Summary:\")\n",
    "print(\"=\" * 80)\n",
    "df_merged.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical columns\n",
    "categorical_cols = df_merged.select_dtypes(include=['object']).columns.tolist()\n",
    "categorical_cols.remove('data_source')  # We already analyzed this\n",
    "\n",
    "print(f\"Categorical features: {len(categorical_cols)}\")\n",
    "print(categorical_cols)\n",
    "\n",
    "print(\"\\nUnique values per categorical feature:\")\n",
    "for col in categorical_cols:\n",
    "    print(f\"  {col}: {df_merged[col].nunique()} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Merged Dataset\n",
    "\n",
    "We'll save the merged dataset in multiple formats for different purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories if they don't exist\n",
    "os.makedirs('../data/raw', exist_ok=True)\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "print(\"Output directories created/verified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to data/raw (as the raw merged version)\n",
    "output_path_raw = '../data/raw/bank_merged_raw.csv'\n",
    "df_merged.to_csv(output_path_raw, index=False)\n",
    "print(f\"✓ Saved raw merged dataset to: {output_path_raw}\")\n",
    "print(f\"  Size: {os.path.getsize(output_path_raw) / (1024*1024):.2f} MB\")\n",
    "\n",
    "# Also save as pickle for faster loading\n",
    "output_path_pickle = '../data/raw/bank_merged_raw.pkl'\n",
    "df_merged.to_pickle(output_path_pickle)\n",
    "print(f\"\\n✓ Saved as pickle to: {output_path_pickle}\")\n",
    "print(f\"  Size: {os.path.getsize(output_path_pickle) / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "### What We Accomplished:\n",
    "\n",
    "✅ Loaded both dataset variants (bank-full.csv and bank-additional-full.csv)  \n",
    "✅ Identified column differences between datasets  \n",
    "✅ Aligned columns by adding missing features with appropriate handling  \n",
    "✅ Added source tracking to maintain data provenance  \n",
    "✅ Successfully merged datasets into single comprehensive dataset  \n",
    "✅ Saved merged dataset in multiple formats  \n",
    "\n",
    "### Key Statistics:\n",
    "\n",
    "- **Total Rows**: 86,399 (45,211 + 41,188)\n",
    "- **Total Features**: 21 (including data_source)\n",
    "- **Target Variable**: Binary (yes/no) - Imbalanced dataset\n",
    "- **Missing Values**: \n",
    "  - Economic indicators (5 features) missing for bank-full records\n",
    "  - Balance and day missing for bank-additional records\n",
    "  - day_of_week missing for bank-full records\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "The merged dataset is now ready for:\n",
    "1. **Exploratory Data Analysis (EDA)** - Notebook 3\n",
    "2. **Feature Engineering** - Notebook 3\n",
    "3. **Model Development** - Notebook 4\n",
    "\n",
    "---\n",
    "\n",
    "**Proceed to Notebook 3 for Exploratory Data Analysis**"
   ]
  }
 ],
 "metadata": {
  "kernelnel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
