{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6: Interpretability & Insights\n\n## Objective\nThis notebook provides comprehensive model interpretability using:\n1. SHAP (SHapley Additive exPlanations) for global and local interpretability\n2. LIME (Local Interpretable Model-agnostic Explanations)\n3. Feature importance analysis\n4. Partial dependence plots\n5. Business insights and actionable recommendations\n\n---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport shap\nimport lime\nimport lime.lime_tabular\nimport warnings\nimport joblib\n\nfrom sklearn.inspection import permutation_importance, partial_dependence\n\nwarnings.filterwarnings('ignore')\nshap.initjs()\nprint('Libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Best Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model (example: XGBoost)\n# best_model = mlflow.<framework>.load_model('runs:/<run_id>/model')\n# Or load from saved file\n# best_model = joblib.load('../models/best_model.pkl')\n\nprint('Load the best performing model from Notebook 5')\nprint('Example: best_model = joblib.load(\"../models/best_model.pkl\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\ndf = pd.read_pickle('../data/interim/bank_with_features.pkl')\nprint(f'Data loaded: {df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tree-based models, get feature importance\ndef plot_feature_importance(model, feature_names, top_n=20):\n    if hasattr(model, 'feature_importances_'):\n        importances = model.feature_importances_\n        indices = np.argsort(importances)[::-1][:top_n]\n        \n        plt.figure(figsize=(10, 8))\n        plt.barh(range(top_n), importances[indices])\n        plt.yticks(range(top_n), [feature_names[i] for i in indices])\n        plt.xlabel('Feature Importance')\n        plt.title(f'Top {top_n} Feature Importances')\n        plt.gca().invert_yaxis()\n        plt.tight_layout()\n        plt.savefig('../reports/figures/feature_importance.png', dpi=300, bbox_inches='tight')\n        plt.show()\n        \n        return pd.DataFrame({'Feature': [feature_names[i] for i in indices],\n                           'Importance': importances[indices]})\n    else:\n        print('Model does not have feature_importances_ attribute')\n        return None\n\nprint('Feature importance plotting function defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SHAP Values - Global Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SHAP explainer\nprint('Creating SHAP explainer...')\n\n# For tree-based models:\n# explainer = shap.TreeExplainer(model)\n# For any model:\n# explainer = shap.KernelExplainer(model.predict_proba, X_train_sample)\n\nprint('Example: explainer = shap.TreeExplainer(best_model)')\nprint('         shap_values = explainer.shap_values(X_test)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Summary Plot\ndef plot_shap_summary(shap_values, X_test, feature_names):\n    plt.figure(figsize=(12, 8))\n    shap.summary_plot(shap_values, X_test, feature_names=feature_names, show=False)\n    plt.tight_layout()\n    plt.savefig('../reports/figures/shap_summary_plot.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\nprint('SHAP summary plot function defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Bar Plot (mean absolute SHAP values)\ndef plot_shap_bar(shap_values, X_test, feature_names):\n    plt.figure(figsize=(10, 8))\n    shap.summary_plot(shap_values, X_test, plot_type='bar', \n                     feature_names=feature_names, show=False)\n    plt.tight_layout()\n    plt.savefig('../reports/figures/shap_bar_plot.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\nprint('SHAP bar plot function defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SHAP Values - Local Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain individual predictions\ndef explain_prediction(explainer, X_instance, feature_names, instance_id):\n    shap_values_instance = explainer.shap_values(X_instance)\n    \n    plt.figure(figsize=(10, 6))\n    shap.waterfall_plot(shap.Explanation(values=shap_values_instance[0], \n                                         base_values=explainer.expected_value,\n                                         data=X_instance[0],\n                                         feature_names=feature_names),\n                       show=False)\n    plt.title(f'SHAP Waterfall Plot - Instance {instance_id}')\n    plt.tight_layout()\n    plt.savefig(f'../reports/figures/shap_waterfall_instance_{instance_id}.png', \n                dpi=300, bbox_inches='tight')\n    plt.show()\n\nprint('Local explanation function defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LIME Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIME explainer\ndef explain_with_lime(model, X_train, X_instance, feature_names, class_names, instance_id):\n    explainer = lime.lime_tabular.LimeTabularExplainer(\n        X_train,\n        feature_names=feature_names,\n        class_names=class_names,\n        mode='classification'\n    )\n    \n    explanation = explainer.explain_instance(\n        X_instance[0],\n        model.predict_proba,\n        num_features=10\n    )\n    \n    # Save explanation\n    explanation.save_to_file(f'../reports/figures/lime_explanation_instance_{instance_id}.html')\n    \n    # Plot\n    fig = explanation.as_pyplot_figure()\n    plt.tight_layout()\n    plt.savefig(f'../reports/figures/lime_plot_instance_{instance_id}.png', \n                dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    return explanation\n\nprint('LIME explanation function defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Permutation Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation importance\ndef calculate_permutation_importance(model, X_test, y_test, feature_names):\n    result = permutation_importance(model, X_test, y_test, \n                                   n_repeats=10, random_state=42, n_jobs=-1)\n    \n    # Sort by importance\n    sorted_idx = result.importances_mean.argsort()[::-1][:20]\n    \n    plt.figure(figsize=(10, 8))\n    plt.barh(range(len(sorted_idx)), result.importances_mean[sorted_idx])\n    plt.yticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx])\n    plt.xlabel('Permutation Importance')\n    plt.title('Top 20 Features by Permutation Importance')\n    plt.gca().invert_yaxis()\n    plt.tight_layout()\n    plt.savefig('../reports/figures/permutation_importance.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    return result\n\nprint('Permutation importance function defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Partial Dependence Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial dependence plots\ndef plot_partial_dependence(model, X, features, feature_names):\n    from sklearn.inspection import PartialDependenceDisplay\n    \n    fig, ax = plt.subplots(figsize=(15, 10))\n    display = PartialDependenceDisplay.from_estimator(\n        model, X, features, feature_names=feature_names,\n        ax=ax, n_cols=3, grid_resolution=50\n    )\n    plt.tight_layout()\n    plt.savefig('../reports/figures/partial_dependence_plots.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\nprint('Partial dependence plot function defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Business Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Findings:\n\n1. **Most Influential Features**:\n   - List top 5-10 features based on SHAP/importance analysis\n   - Interpret their business meaning\n\n2. **Feature Interactions**:\n   - Identify important feature combinations\n   - Explain synergistic effects\n\n3. **Client Segmentation**:\n   - High-value segments more likely to subscribe\n   - Low-risk segments to avoid\n\n4. **Optimal Contact Strategy**:\n   - Best timing for contacts\n   - Optimal frequency\n   - Preferred communication channels\n\n5. **Economic Context Impact**:\n   - Effect of economic indicators\n   - Market conditions influence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Actionable Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marketing Strategy Recommendations:\n\n1. **Target Prioritization**:\n   - Focus on clients with high predicted probability\n   - Characteristics of high-conversion clients\n\n2. **Contact Optimization**:\n   - Optimal number of contacts\n   - Best times to call (day/month)\n   - Personalize approach based on client profile\n\n3. **Resource Allocation**:\n   - Allocate more resources to high-potential segments\n   - Reduce efforts on low-conversion segments\n\n4. **Product Design**:\n   - Tailor term deposit products to different segments\n   - Pricing strategies based on client characteristics\n\n5. **Risk Management**:\n   - Monitor economic indicators\n   - Adjust campaigns based on market conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n\nThis notebook provided comprehensive model interpretability:\n\n\u2705 Feature importance analysis  \n\u2705 SHAP global and local explanations  \n\u2705 LIME for individual predictions  \n\u2705 Permutation importance  \n\u2705 Partial dependence plots  \n\u2705 Business insights derived from model  \n\u2705 Actionable recommendations for marketing  \n\n---\n\n**Proceed to Notebook 7 for Critical Reflection**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}