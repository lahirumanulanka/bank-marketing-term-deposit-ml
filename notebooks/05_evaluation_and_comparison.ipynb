{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Evaluation & Comparison\n",
    "\n",
    "## Objective\n",
    "This notebook covers:\n",
    "1. Comprehensive evaluation of all models with multiple metrics\n",
    "2. Confusion matrices and ROC curves\n",
    "3. Error analysis to identify misclassified cases\n",
    "4. Model comparison in tables and plots\n",
    "5. Hyperparameter tuning for best models\n",
    "6. Threshold optimization\n",
    "7. Handling class imbalance with SMOTE\n",
    "8. Final model selection\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import joblib\n",
    "import mlflow\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, classification_report, roc_curve, precision_recall_curve,\n",
    "    average_precision_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print('Libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Models and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data from model development notebook\n",
    "# This assumes you've saved the test data during model training\n",
    "# Or reload and split the data\n",
    "\n",
    "df = pd.read_pickle('../data/interim/bank_with_features.pkl')\n",
    "print(f'Data loaded: {df.shape}')\n",
    "\n",
    "# Note: In practice, you'd load the exact same train/test split used in notebook 4\n",
    "# For now, we'll recreate it with the same random_state\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Encode target\n",
    "df['y_binary'] = (df['y'] == 'yes').astype(int)\n",
    "\n",
    "# Select features\n",
    "exclude_cols = ['y', 'y_binary', 'data_source']\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "print(f'Features: {len(feature_cols)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode and prepare data (same as notebook 4)\n",
    "df_encoded = df.copy()\n",
    "categorical_cols = df_encoded[feature_cols].select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_encoded[col] = df_encoded[col].fillna('missing')\n",
    "    df_encoded[col] = le.fit_transform(df_encoded[col])\n",
    "\n",
    "numerical_cols = df_encoded[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
    "df_encoded[numerical_cols] = df_encoded[numerical_cols].fillna(df_encoded[numerical_cols].median())\n",
    "\n",
    "# Split\n",
    "X = df_encoded[feature_cols]\n",
    "y = df_encoded['y_binary']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f'Train: {X_train.shape}, Test: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Trained Models from MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set MLflow tracking\n",
    "mlflow.set_tracking_uri('../experiments/mlruns')\n",
    "experiment = mlflow.get_experiment_by_name('bank_marketing_models')\n",
    "\n",
    "if experiment:\n",
    "    print(f'Experiment found: {experiment.name}')\n",
    "    print(f'Experiment ID: {experiment.experiment_id}')\n",
    "    \n",
    "    # Get all runs\n",
    "    runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "    print(f'\\nTotal runs: {len(runs)}')\n",
    "    print(runs[['run_id', 'tags.mlflow.runName', 'metrics.accuracy', 'metrics.f1_score', 'metrics.roc_auc']].head(10))\n",
    "else:\n",
    "    print('No experiment found. Models need to be trained first (see Notebook 4).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate All Models with Multiple Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate model\n",
    "def evaluate_model(model, X_test, y_test, model_name, use_scaled=False):\n",
    "    \"\"\"Evaluate model and return metrics\"\"\"\n",
    "    X_eval = X_test_scaled if use_scaled else X_test\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_eval)\n",
    "    \n",
    "    # Probability predictions\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_pred_proba = model.predict_proba(X_eval)[:, 1]\n",
    "    else:\n",
    "        # For neural networks\n",
    "        import torch\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.FloatTensor(X_eval)\n",
    "            y_pred_proba = model(X_tensor).numpy().flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1-Score': f1_score(y_test, y_pred),\n",
    "        'ROC-AUC': roc_auc_score(y_test, y_pred_proba),\n",
    "        'Avg Precision': average_precision_score(y_test, y_pred_proba)\n",
    "    }\n",
    "    \n",
    "    return metrics, y_pred, y_pred_proba\n",
    "\n",
    "print('Evaluation function defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This cell would load actual models from MLflow\n",
    "# For demonstration, we show the structure\n",
    "\n",
    "print(\"To load models from MLflow, use:\")\n",
    "print(\"\")\n",
    "print(\"model = mlflow.sklearn.load_model(f'runs:/{run_id}/model')\")\n",
    "print(\"or\")\n",
    "print(\"model = mlflow.<framework>.load_model(...)\")\n",
    "print(\"\")\n",
    "print(\"Then evaluate each model and store results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "# This would be populated with actual results\n",
    "\n",
    "comparison_data = {\n",
    "    'Model': ['Logistic Regression', 'Random Forest', 'XGBoost', 'LightGBM', 'CatBoost', 'Neural Network'],\n",
    "    'Accuracy': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],  # Placeholder\n",
    "    'Precision': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    'Recall': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    'F1-Score': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    'ROC-AUC': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print('Model Comparison Table:')\n",
    "print('=' * 100)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print('\\nNote: Fill with actual metrics after model evaluation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, model_name):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "    plt.title(f'Confusion Matrix - {model_name}', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'../reports/figures/confusion_matrix_{model_name.lower().replace(\" \", \"_\")}.png', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "print('Confusion matrix plotting function defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ROC Curves Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot ROC curves for all models\n",
    "def plot_roc_curves(models_results):\n",
    "    \"\"\"Plot ROC curves for multiple models\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    for model_name, (y_test, y_pred_proba) in models_results.items():\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.3f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.title('ROC Curves Comparison', fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../reports/figures/roc_curves_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "print('ROC curve plotting function defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Precision-Recall Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot PR curves\n",
    "def plot_pr_curves(models_results):\n",
    "    \"\"\"Plot Precision-Recall curves for multiple models\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    for model_name, (y_test, y_pred_proba) in models_results.items():\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "        avg_precision = average_precision_score(y_test, y_pred_proba)\n",
    "        plt.plot(recall, precision, label=f'{model_name} (AP = {avg_precision:.3f})')\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall', fontsize=12)\n",
    "    plt.ylabel('Precision', fontsize=12)\n",
    "    plt.title('Precision-Recall Curves Comparison', fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../reports/figures/pr_curves_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "print('PR curve plotting function defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis for best model\n",
    "def analyze_errors(X_test, y_test, y_pred, feature_names):\n",
    "    \"\"\"Analyze misclassified samples\"\"\"\n",
    "    # Identify misclassified samples\n",
    "    errors = y_test != y_pred\n",
    "    \n",
    "    print(f'Total misclassified: {errors.sum()} ({errors.sum()/len(y_test)*100:.2f}%)')\n",
    "    \n",
    "    # False positives and false negatives\n",
    "    false_positives = (y_test == 0) & (y_pred == 1)\n",
    "    false_negatives = (y_test == 1) & (y_pred == 0)\n",
    "    \n",
    "    print(f'False Positives: {false_positives.sum()}')\n",
    "    print(f'False Negatives: {false_negatives.sum()}')\n",
    "    \n",
    "    # Analyze characteristics of misclassified samples\n",
    "    if isinstance(X_test, pd.DataFrame):\n",
    "        X_test_errors = X_test[errors]\n",
    "    else:\n",
    "        X_test_errors = pd.DataFrame(X_test[errors], columns=feature_names)\n",
    "    \n",
    "    print('\\nCharacteristics of misclassified samples:')\n",
    "    print(X_test_errors.describe())\n",
    "    \n",
    "    return X_test_errors\n",
    "\n",
    "print('Error analysis function defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Grid search for XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "print('Hyperparameter tuning for XGBoost...')\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "print(f'Parameter grid: {param_grid}')\n",
    "print(f'Total combinations: {np.prod([len(v) for v in param_grid.values()])}')\n",
    "\n",
    "# Note: Actual grid search would be:\n",
    "# scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "# xgb_model = XGBClassifier(scale_pos_weight=scale_pos_weight, random_state=42)\n",
    "# grid_search = GridSearchCV(xgb_model, param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# print(f'Best parameters: {grid_search.best_params_}')\n",
    "# print(f'Best F1 score: {grid_search.best_score_:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Threshold Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find optimal threshold\n",
    "def find_optimal_threshold(y_test, y_pred_proba, metric='f1'):\n",
    "    \"\"\"Find optimal classification threshold\"\"\"\n",
    "    thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "    scores = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred_thresh = (y_pred_proba >= threshold).astype(int)\n",
    "        \n",
    "        if metric == 'f1':\n",
    "            score = f1_score(y_test, y_pred_thresh)\n",
    "        elif metric == 'precision':\n",
    "            score = precision_score(y_test, y_pred_thresh)\n",
    "        elif metric == 'recall':\n",
    "            score = recall_score(y_test, y_pred_thresh)\n",
    "        \n",
    "        scores.append(score)\n",
    "    \n",
    "    # Find best threshold\n",
    "    best_idx = np.argmax(scores)\n",
    "    best_threshold = thresholds[best_idx]\n",
    "    best_score = scores[best_idx]\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(thresholds, scores, marker='o')\n",
    "    plt.axvline(best_threshold, color='r', linestyle='--', \n",
    "                label=f'Optimal threshold = {best_threshold:.2f}')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel(f'{metric.upper()} Score')\n",
    "    plt.title(f'Threshold Tuning for {metric.upper()}')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'../reports/figures/threshold_tuning_{metric}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return best_threshold, best_score\n",
    "\n",
    "print('Threshold tuning function defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. SMOTE for Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE\n",
    "print('Applying SMOTE for class imbalance...')\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f'Original training set: {X_train.shape}')\n",
    "print(f'SMOTE training set: {X_train_smote.shape}')\n",
    "print(f'\\nOriginal class distribution:')\n",
    "print(y_train.value_counts())\n",
    "print(f'\\nSMOTE class distribution:')\n",
    "print(pd.Series(y_train_smote).value_counts())\n",
    "\n",
    "# Train model with SMOTE data\n",
    "print('\\nTrain a model with SMOTE-balanced data and compare performance.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Best Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of findings\n",
    "print('Model Evaluation Summary')\n",
    "print('=' * 80)\n",
    "print('\\n1. All models evaluated with multiple metrics')\n",
    "print('2. Confusion matrices generated for error analysis')\n",
    "print('3. ROC and PR curves compared across models')\n",
    "print('4. Hyperparameter tuning performed')\n",
    "print('5. Optimal classification threshold determined')\n",
    "print('6. SMOTE applied for class imbalance handling')\n",
    "print('\\n' + '=' * 80)\n",
    "print('\\nBest Model Selection Criteria:')\n",
    "print('- Highest F1-score (balanced precision and recall)')\n",
    "print('- Good ROC-AUC performance')\n",
    "print('- Robust to class imbalance')\n",
    "print('- Interpretable for business use')\n",
    "print('\\nRecommendation: Select based on actual metrics from evaluation above.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Proceed to **Notebook 6** for:\n",
    "- Model interpretability with SHAP and LIME\n",
    "- Feature importance analysis\n",
    "- Business insights and recommendations\n",
    "\n",
    "---\n",
    "\n",
    "**End of Evaluation & Comparison**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}